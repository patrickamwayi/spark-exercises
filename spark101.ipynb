{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe0a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 11:05:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydataset\n",
    "from pydataset import data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fab6ba",
   "metadata": {},
   "source": [
    "## EXERCISE\n",
    "### Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "### a)The name of the column should be language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6466e39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SQL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JavaHTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language\n",
       "0      Python\n",
       "1         SQL\n",
       "2  JavaScript\n",
       "3    JavaHTML\n",
       "4           R"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating pd dataframe with my favorite programming languages\n",
    "df = pd.DataFrame ({ 'language': ['Python', 'SQL','JavaScript', 'Java' 'HTML', 'R']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbf8c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  language|\n",
      "+----------+\n",
      "|    Python|\n",
      "|       SQL|\n",
      "|JavaScript|\n",
      "|  JavaHTML|\n",
      "|         R|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating spark dataframe with my favorite programming languages\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37682dd1",
   "metadata": {},
   "source": [
    "### b) View the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9f6778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output schema of dataframe\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead4409",
   "metadata": {},
   "source": [
    "### c) Output the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24cdafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "#checking the shape of the spark dataframe\n",
    "print((spark_df.count(), len(spark_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8bc24",
   "metadata": {},
   "source": [
    "### d) Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462ccb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  language|\n",
      "+----------+\n",
      "|    Python|\n",
      "|       SQL|\n",
      "|JavaScript|\n",
      "|  JavaHTML|\n",
      "|         R|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check spark df\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabec9ca",
   "metadata": {},
   "source": [
    "### 2. Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "### a) Create 1 column of output that contains a message like the one below:\n",
    " - The 1999 audi a4 has a 4 cylinder engine.\n",
    " - For each vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea671e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, concat, lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa933df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define mpg df\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9d9aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|column                                                        |\n",
      "+--------------------------------------------------------------+\n",
      "|The 1999 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 1999 audi a6 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a6 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a6 quattro has a 8 cylinder engine.             |\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine.|\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine.|\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#function to create column output\n",
    "spark_df = mpg.select(concat(lit('The '), \n",
    "                       mpg.year,\n",
    "                       lit(' '),\n",
    "                        mpg.manufacturer,\n",
    "                       lit(' '), \n",
    "                       mpg.model, \n",
    "                       lit(' has a '),\n",
    "                       mpg.cyl, \n",
    "                       lit(' cylinder engine.')).alias(\"column\"))\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c11c4",
   "metadata": {},
   "source": [
    "### b) Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "912fd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9180d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------+\n",
      "|     trans|regexp_extract(trans, (\\w{4,6}), 1)|\n",
      "+----------+-----------------------------------+\n",
      "|  auto(l5)|                               auto|\n",
      "|manual(m5)|                             manual|\n",
      "|manual(m6)|                             manual|\n",
      "|  auto(av)|                               auto|\n",
      "|  auto(l5)|                               auto|\n",
      "+----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mpg.select(\n",
    "    \"trans\",\n",
    "    regexp_extract('trans',r'(\\w{4,6})',1)\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a2a92",
   "metadata": {},
   "source": [
    "### 3. Load the tips dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83668a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load tips dataframe showing first 5 rows\n",
    "spark_df = spark.createDataFrame(pydataset.data('tips'))\n",
    "spark_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efc381",
   "metadata": {},
   "source": [
    "### a) What percentage of observations are smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04eb9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38114754098360654"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate percentage of observations that are smokers.\n",
    "spark_df.where(spark_df.smoker == 'Yes').count() / spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3ba893",
   "metadata": {},
   "source": [
    "### b) Create a column that contains the tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b2b371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|     tip_percentage|\n",
      "+----------+----+------+------+---+------+----+-------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|0.05944673337257211|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|0.16054158607350097|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|0.16658733936220846|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2| 0.1397804054054054|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|0.14680764538430255|\n",
      "+----------+----+------+------+---+------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a tip percentage column.\n",
    "spark_df = spark_df.withColumn('tip_percentage', spark_df.tip / spark_df.total_bill)\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2ce7a",
   "metadata": {},
   "source": [
    "### c) Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e0fcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|   sex|smoker|avg(tip_percentage)|\n",
      "+------+------+-------------------+\n",
      "|  Male|    No| 0.1606687151291298|\n",
      "|Female|    No| 0.1569209707691836|\n",
      "|  Male|   Yes|0.15277117520248512|\n",
      "|Female|   Yes|0.18215035269941032|\n",
      "+------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate average tip percentage for sex and smoker groups.\n",
    "spark_df.groupBy('sex', 'smoker').agg(F.mean(spark_df.tip_percentage)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0acdf4",
   "metadata": {},
   "source": [
    "### 4. Use the seattle weather dataset referenced in the lesson to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "915e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29a132c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "|               date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01 00:00:00|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02 00:00:00|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03 00:00:00|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04 00:00:00|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05 00:00:00|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create seattle weather dataframe\n",
    "spark_df = spark.createDataFrame(data.seattle_weather())\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30430c1",
   "metadata": {},
   "source": [
    "### a) Convert the temperatures to fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e61cca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------------------+--------+----+-------+\n",
      "|               date|precipitation|          temp_max|temp_min|wind|weather|\n",
      "+-------------------+-------------+------------------+--------+----+-------+\n",
      "|2012-01-01 00:00:00|          0.0|55.040000000000006|    41.0| 4.7|drizzle|\n",
      "|2012-01-02 00:00:00|         10.9|             51.08|   37.04| 4.5|   rain|\n",
      "|2012-01-03 00:00:00|          0.8|             53.06|   44.96| 2.3|   rain|\n",
      "|2012-01-04 00:00:00|         20.3|             53.96|   42.08| 4.7|   rain|\n",
      "|2012-01-05 00:00:00|          1.3|48.019999999999996|   37.04| 6.1|   rain|\n",
      "+-------------------+-------------+------------------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert temperatures to fahrenheit.\n",
    "spark_df = spark_df.withColumn('temp_max', spark_df.temp_max * 1.8 + 32).withColumn('temp_min', spark_df.temp_min * 1.8 + 32)\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117cd2dd",
   "metadata": {},
   "source": [
    "### b) Which month has the most rain, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b66d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(month=11, avg_total_rain=210.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the month with most rain on average.\n",
    "spark_df.withColumn('month', F.month('date'))\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .where(spark_df.weather == 'rain')\\\n",
    "    .groupBy('month', 'year')\\\n",
    "    .agg(F.sum('precipitation').alias('total_rain'))\\\n",
    "    .groupBy('month')\\\n",
    "    .agg(F.mean('total_rain').alias('avg_total_rain'))\\\n",
    "    .sort('avg_total_rain', ascending = False).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1477a",
   "metadata": {},
   "source": [
    "### c) Which year was the windiest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4296355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|total_winds|\n",
      "+----+-----------+\n",
      "|2012|     1244.7|\n",
      "+----+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating windiest year.\n",
    "spark_df.withColumn('year', F.year('date'))\\\n",
    "    .groupBy('year')\\\n",
    "    .agg(F.sum('wind').alias('total_winds'))\\\n",
    "    .sort('total_winds', ascending = False).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c77247",
   "metadata": {},
   "source": [
    "### d) What is the most frequent type of weather in January?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "becde717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==========================================================(8 + 0) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|weather|count|\n",
      "+-------+-----+\n",
      "|    fog|   38|\n",
      "+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating most frequent type of weather in January.\n",
    "spark_df.where(F.month('date') == 1)\\\n",
    "    .groupBy('weather')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending = False).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8262c",
   "metadata": {},
   "source": [
    "### e) What is the average high and low temperature on sunny days in July in 2013 and 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "990a3f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|    avg(temp_max)|    avg(temp_min)|\n",
      "+-----------------+-----------------+\n",
      "|80.29192307692308|57.52884615384615|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating the average high and low temperature on sunny days in July in 2013 and 2014\n",
    "spark_df.where((spark_df.weather == 'sun')\\\n",
    "         & (F.month('date') == 7)\\\n",
    "         & ((F.year('date') == 2013)\\\n",
    "         | (F.year('date') == 2014)))\\\n",
    "    .select(F.mean('temp_max'), F.mean('temp_min')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761394b4",
   "metadata": {},
   "source": [
    "### f) What percentage of days were rainy in q3 of 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b6dceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/21 11:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/21 11:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/21 11:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/21 11:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/21 11:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/21 11:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/10/21 11:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-------+-----+--------------------+\n",
      "|weather|count|          percentage|\n",
      "+-------+-----+--------------------+\n",
      "|    fog|   21| 0.22826086956521738|\n",
      "|drizzle|    5| 0.05434782608695652|\n",
      "|   rain|    2|0.021739130434782608|\n",
      "|    sun|   64|  0.6956521739130435|\n",
      "+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating percentage of days that were rainy in q3 of 2015\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark_df.where((F.year('date') == 2015) & (F.quarter('date') == 3))\\\n",
    "    .groupBy('weather')\\\n",
    "    .count()\\\n",
    "    .withColumn('percentage', F.col('count') / F.sum('count').over(Window.partitionBy())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885abc37",
   "metadata": {},
   "source": [
    "### g) For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3358e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+\n",
      "|year|avg(non_zero_precipitation)|\n",
      "+----+---------------------------+\n",
      "|2012|        0.48360655737704916|\n",
      "|2013|        0.41643835616438357|\n",
      "|2014|          0.410958904109589|\n",
      "|2015|        0.39452054794520547|\n",
      "+----+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating percentage of days it rained (had non-zero precipitation).\n",
    "\n",
    "spark_df.withColumn('year', F.year('date'))\\\n",
    "    .withColumn('non_zero_precipitation', (spark_df.precipitation > 0).cast('int'))\\\n",
    "    .groupBy('year')\\\n",
    "    .agg(F.mean('non_zero_precipitation')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
